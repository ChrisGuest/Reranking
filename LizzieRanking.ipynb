{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Aggregation Methods\n",
    "\n",
    "We want to rank answers so that the best content floats to the top. The information we get is users' ratings of the answers. We must aggregate users' ratings in two ways: \n",
    "\n",
    "1. A single user will rate multiple aspects of a single answer (within-rating aggregation)\n",
    "2. Multiple users rate a given answer (between-rating aggregation)\n",
    "\n",
    "I'm focusing on the second kind of aggregation. Currently, SWARM just averages the scores.\n",
    "\n",
    "## Rating honestly vs. strategically\n",
    "\n",
    "We've noticed that as the deadline approaches, people are giving items more extreme scores (closer to 0 or 100) in order to move the average further, and get the item to the top or bottom of the pile. This is a sensible voting strategy. If I see that an item has a current score of 70 but believe it should have a score of 85, then I should give it a rating of 100 rather than 85, to move it closer to the correct score.\n",
    "\n",
    "However, this voting strategy causes problems. It means our data on ratings is next to useless - we don't learn users' true opinion of the answer quality. It's also undemocratic. A user who gives an extreme rating has more leverage than a user who gives a rating close to the current average. \n",
    "\n",
    "Ideally, SWARM would use an aggregation method that elicits users' honest opinions of the answers, giving helpful feedback to the answer author and helpful data to the analytics team. Ideally we would rank items in a way that doesn't privilege one rater over another.\n",
    "\n",
    "## Rank aggregation & impossibility theorems\n",
    "\n",
    "To remove the incentive to gve extreme scores, the obvious solution is to use a rank aggregation method. Each user rates several answers, giving each a score out of 100 (after within-rating aggregation), but we don't need to use the exact numbers. Each rater's list of scores can be coarsened into a rank order of preferences. We can then use a standard method for aggregating ranks rather than real-valued scores.\n",
    "\n",
    "Unfortunately, the [Gibbard-Satterthwaite Theorem](https://en.wikipedia.org/wiki/Gibbard%E2%80%93Satterthwaite_theorem) shows that rank aggregation methods cannot satisfy our desiderata: all \"strategy-proof\" methods for ranking more than two options must give all the power to a single rater. \n",
    "\n",
    "This might be particularly problematic for SWARM. Many voting systems are _robust_ to strategic voting, because the strategic voter needs lots of information about other voters' preferences. SWARM doesn't tell you exactly how other people are rating, but it lets you _change your own rating_ an arbitrary number of times. Users who wish to vote strategically could try different ratings until they achieve their desired outcome. \n",
    "\n",
    "[The space of possible rankings is large](https://en.wikipedia.org/wiki/Ordered_Bell_number), so if there are five or more answers, users cannot explore the space of rankings in a reasonable time. Even exploring a subset would take much more effort than just giving an extreme score. But it is an open question how often users would vote strategically if we aggregated answers like this.\n",
    "\n",
    "The Gibbard-Satterthwaite Theorem tells us there is no perfect system, but we may be able to find a decent option - a system that produces something close to the ideal ranking in most cases, and which can't be manipulated without spending a lot of time and effort.\n",
    "\n",
    "## Evaluating performance\n",
    "\n",
    "The \"ideal ranking\" is undefined for any of our real examples, but we have some intuitions about which answers are best. I propose testing each rank aggregation method by looking at how it ranks the answers to **past questions** on SWARM Beta. A good method will accord pretty well with our intuitions; a bad one will not. However, this evaluation only gives us information about a small number of cases, and doesn't tell us about robustness to strategies other than \"give extreme scores\".\n",
    "\n",
    "We can also evaluate performance using **synthetic data**. We randomly generate answers of varying quality, and sets of ratings that are realistically sparse and noisy. We apply the rank aggregation methods to this data, and see how close the overal ranking comes to the true rank order of the answers based on their quality. \n",
    "\n",
    "Because we can sample repeatedly, evaluation on synthetic data tells us about the average case performance of the ranking methods. However, it doesn't tell us about robustness to particular strategies (unless we build those strategies into the rating generation method, which would take a lot of work).\n",
    "\n",
    "## Ranking Methods\n",
    "\n",
    "I implemented and tested the following ranking methods:\n",
    "\n",
    "1. Rank of average score (status quo in SWARM)\n",
    "2. RdR's ranking method (\"place everything with more than one rating above everthing with one rating. Within the two groups, rank by average score\")\n",
    "3. [Borda count](https://en.wikipedia.org/wiki/Borda_count) (\"For each rater, for each answer, count how many other answers it _outranks_. Then for each answer, sum this count across all raters\")\n",
    "4. User Preference Ranking. This uses the exact score rather than just the rank, but it aggregates pairwise comparisons of items, among the set of users who have rated both items in the pair. It is more robust to sparse ranking. However it is still vulnerable to user manipultion using extreme scores. User Preference Ranking is described in Chapter 10 of _Who's #1?_ by Langville & Meyer.\n",
    "5. Anca's ranking method: essentially User Preference Ranking, but instead of the exact point difference between two answers, I just ask which one \"won\" according to that rater. The proportion of wins vs losses, among raters who rated both answers, is the pairwise difference between those two answers. Because it only considers rank differences, Anca's method is not vulnerable to manipulation by giving extreme scores.\n",
    "6. Median score\n",
    "7. Geometric mean\n",
    "8. Sum\n",
    "8. Borda, normalized so that every rater's ratings add to 1 (\"Tim ranking\")\n",
    "9. Borda, normalized so that every rater's ratings has maximum 1 (\"Tim2 ranking\")\n",
    "10. Lizzie ranking: test whether Anca ranking correlates with number of ratings per answer. If the correlation is non-significant or negative, return Anca ranknig vector. If significant and positive, add some weight for the number of rankings the answer received, where the weight is proportional to the size of the correlation.\n",
    "\n",
    "All these methods start by creating a rater-by-answer matrix of ratings.\n",
    "\n",
    "### Simulation study\n",
    "\n",
    "I started by generating synthetic data. I varied the following parameters:\n",
    "\n",
    "* ```rnoise```, a scaling factor that increases the amount of noise in the ratings\n",
    "* ```punrated```, the probability that a given rating will be hidden\n",
    "* ```pslacker```, the probability that a user will fail to submit any ratings \n",
    "* ```plurker```, the probability that a user will fail to submit an answer\n",
    "* ```toprated```: if this is True, the user will rate the answers they consider best, and fail to rate the answers they consider worse. \n",
    "    * Note: There is no set number of ratings the user completes. The number of ratings for each user, *k*, is drawn from a binomial with probability = ```1 - punrated```. Whether those *k* ratings are then distributed uniformly, or the user rates their *k* top answers, depends on ```toprated```.\n",
    "* ```enoise```: Each user has an \"easiness\" bias parameter that is drawn from a Gaussian, reflecting that some users are harsher raters and some are easier. ```enoise``` is the standard deviation of that Gaussian.\n",
    "\n",
    "### Results\n",
    "\n",
    "I found that ```toprated``` had a big influence on which methods worked best. When ```toprated``` was True, the Borda count and related methods all did best. When it was False, all the other methods did about equally well, and the Borda count methods did much worse.\n",
    "\n",
    "I believe this is because Borda count essentially awards a bonus for each rating. Each rating can only improve an item's Borda count. By contrast, using mean_rating(), an additional rating can *hurt* an item's mean score; likewise with User Preferences, Anca Rating, etc. If users are only rating the best items, then \"receiving lots of ratings\" is a meaningful signal that should be reflected in the score. However if users rate a random selection of items, then \"receiving lots of ratings\" means nothing and should not be rewarded.\n",
    "\n",
    "The only method that performed pretty well in both conditions was Lizzie ranking. It does well because it tries to *detect* whether people are rating the top answers or not, and adjusts the score accordingly. Lizzie Rating didn't do as well as Borda count when ```toprated``` was True, but it did better than any non-Borda method, and when ```toprated``` was False it far outperformed Borda count. On the basis of these results I selected Lizzie ranking as the method to use. \n",
    "\n",
    "### Appendix: Methods I did NOT implement:\n",
    "\n",
    "Evan Miller's [confidence interval-based method](http://www.evanmiller.org/how-not-to-sort-by-average-rating.html), the [Bayesian version](http://www.evanmiller.org/bayesian-average-ratings.html), the [star-rating version](http://www.evanmiller.org/ranking-items-with-star-ratings.html), and [Paul Masurel's variation](https://fulmicoton.com/posts/bayesian_rating/) take the _uncertainty_ around the average score into account. They rank by the lower bound of a confidence interval or credible interval around the average score, or smooth the average using a pseudocount. Unfortunately these methods are still centered on the mean, so they would still be vulnerable to the strategy of \"give extreme scores\". Users who give extremely low scores would have more power than before. Furthermore, we are unlikely to ever get enough ratings for the intervals to become narrow.\n",
    "\n",
    "Minimum Violation Ranking (MVR), as described in Chapter 15 of _Who's #1?_ by Langville & Meyer, would take a set of users' rankings and produce an aggregate ranking that disagrees as little as possible with the input rankings. This method is optimal among all methods that use only rank data (as opposed to, for example, \"average score\" or \"user preference rankings\" which use the exact scores rather than just the ranks). It would do as well as or better than Borda count. However, the exact version of MVR requires solving an integer linear program  every time the scores are updated, which can take a long time. Langville & Meyer show you can speed it up by first solving a relaxed version of the problem. \n",
    "\n",
    "I chose not to implement MVR because (a) the implementation would be moderately complicated, (b) the runtime might be prohibitive, and (c) it's designed for aggregating complete rankings, not partial rankings. We would first turn the partial rankings into complete rankings by placing all unranked items _last_ in each user's ranking. So if I haven't had time to rate an answer, that is indistinguishable from me saying it is the worst answer. Given how sparse our data is, this is a very common case, so I think MVR would work poorly on our data.\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "For evaluating the methods on synthetic data, I have developed a class that pulls the data from a CSV of a synthetic rater-by-answer matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # need this on my laptop because my python & anaconda installation is all kinds of messed up\n",
    "# import sys\n",
    "# sys.path.append(\"/Users/lizzie/anaconda/\")\n",
    "# sys.path.append(\"/Users/lizzie/Library/Python/3.6/lib/python/site-packages\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define abstract score matrix class\n",
    "\n",
    "This class implements the ranking methods. It's abstract because the _data_ for a score matrix could either be pulled from the SWARM DB, or generated synthetically, and I want to separate those cases. Those concrete classes are defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "class AbstractScoreMatrix(object):\n",
    "    \"\"\"A rater-by-answer matrix of scores for answers to the question.\n",
    "    \n",
    "    Includes several ranking methods:\n",
    "        (e) anca_rating()\n",
    "        (j) lizzie_rating()\n",
    "        \n",
    "    All of them return a 1D Numpy array with the *scores* of the answers.\n",
    "    \n",
    "    To implement these methods you need a user-by-answer matrix, where each cell contains a  \n",
    "    single number representing that user's rating of that answer. Ratings are between 0 and 100. \n",
    "    \n",
    "    If the user has not rated an item, I use -1 as the \"missing data\" value.\n",
    "    \n",
    "    This is an abstract class because the instantiation differs depending on where you get the  \n",
    "    data from (a CSV file, a connection to the SWARM database + a question ID, etc.).\n",
    "    \"\"\" \n",
    "    __metaclass__ = ABCMeta\n",
    "    \n",
    "    def get_missing_subset(self):\n",
    "        \"\"\"Return a dictionary of the indices of answers that have been scored and answers that  \n",
    "        haven't (and raters that have provided scores, and raters that havent).\n",
    "        Used in a preprocessing step in user_preferences(), to get a subset of the score matrix \n",
    "        where unrated items have been dropped (and useless raters have been dropped).\n",
    "        Retaining a list of indices of the dropped items means we can still create an overall  \n",
    "        ordering of all items (rated and unrated).\n",
    "        \"\"\"\n",
    "        \n",
    "        subset = {'answers':{'data':np.where(np.logical_not(np.all(self.matrix == -1, \n",
    "                                                                   axis=0)))[0], \\\n",
    "                             'missing':np.where(np.all(self.matrix == -1, axis=0))[0]}, \\\n",
    "                  'raters':{'data':np.where(np.logical_not(np.all(self.matrix == -1, \n",
    "                                                                  axis=1)))[0], \\\n",
    "                            'missing':np.where(np.all(self.matrix == -1, axis=1))[0]}}\n",
    "        return subset\n",
    "    \n",
    "    def anca_rating(self):\n",
    "        \"\"\"modified version of user preference rankings, suggested by Anca Hanea.\n",
    "        \n",
    "        If a particular rater prefers Answer A to Answer B, that is counted as\n",
    "        a win for Answer A. \n",
    "        \n",
    "        The pairwise score difference for Answer A, relative to Answer B, is \n",
    "        (# wins - # losses) / (# of raters who have rated both A and B).\n",
    "        \n",
    "        Answer A's *overall* score is the sum of its pairwise differences with\n",
    "        all the other answers.\n",
    "        \n",
    "        This means some answers get negative scores, while answers that recieve \n",
    "        no ratings at all would score better (0)! To avoid this, I shift up the\n",
    "        scores so every answer that has a rating is at least one point higher \n",
    "        than the unrated answers.\n",
    "        \"\"\"\n",
    "        \n",
    "        # start by excluding the answers that have no ratings and the raters\n",
    "        # that didn't submit any ratings\n",
    "        subset = {'answers':{'data':np.where(np.logical_not(np.all(self.matrix == -1, \n",
    "                                                                   axis=0)))[0], \\\n",
    "                             'missing':np.where(np.all(self.matrix == -1, axis=0))[0]}, \\\n",
    "                  'raters':{'data':np.where(np.logical_not(np.all(self.matrix == -1, \n",
    "                                                                  axis=1)))[0], \\\n",
    "                            'missing':np.where(np.all(self.matrix == -1, axis=1))[0]}}\n",
    "        \n",
    "        m = len(subset[\"answers\"][\"data\"])\n",
    "        \n",
    "        # if no answers have been rated, or no raters have rated anything\n",
    "        if (m < 1 or len(self.subset[\"raters\"][\"data\"]) < 1) or np.all(self.matrix == -1):\n",
    "            return np.zeros((self.matrix.shape[1]))\n",
    "        \n",
    "        score_mat = self.matrix[subset['raters']['data'][:,None], \n",
    "                                subset['answers']['data']]\n",
    "        \n",
    "        # create and fill the matrix of pairwise differences\n",
    "        kmat = np.zeros((m,m))\n",
    "        \n",
    "        for i in range(m):\n",
    "            for j in range(i+1, m):\n",
    "                \n",
    "                # find the rows where both answers were rated\n",
    "                both_rated = np.all((score_mat[:,[i,j]] > 0), axis=1)\n",
    "                \n",
    "                if np.any(both_rated):\n",
    "                    num_pairs = sum(both_rated)\n",
    "                    both_rated = np.where(both_rated)\n",
    "                    \n",
    "                    wins = np.sum(score_mat[both_rated, i] > score_mat[both_rated, j]) \n",
    "                    losses = np.sum(score_mat[both_rated, j] > score_mat[both_rated, i]) \n",
    "                    kmat[i,j] = (wins - losses) / num_pairs\n",
    "                    kmat[j,i] = (losses - wins) / num_pairs\n",
    "        \n",
    "        # sum the pairwise differences to get overall scores\n",
    "        rvec = np.sum(kmat,axis=1)/m\n",
    "        \n",
    "        # normalize, so everything that has received a rating is at least one point higher \n",
    "        # than unrated items\n",
    "        rvec = rvec - np.min(rvec - 1)\n",
    "        \n",
    "        # Insert the rated items into a vector containing scores for all items\n",
    "        all_rvec = np.zeros((len(self.answer_ids)))\n",
    "        for i in range(len(rvec)):\n",
    "            all_rvec[subset[\"answers\"][\"data\"][i]] = rvec[i]\n",
    "            \n",
    "        return all_rvec\n",
    "    \n",
    "    def lizzie_rating(self):\n",
    "        num_ratings = np.sum(self.matrix >= 0, axis = 0)\n",
    "        anca_vec = self.anca_rating()\n",
    "        \n",
    "        test_tau, test_p_value = stats.kendalltau(anca_vec, num_ratings)\n",
    "        \n",
    "        # Conditions to exclude:\n",
    "        # If either vector is all the same value (e.g. [2,2,2]), the test returns nan.\n",
    "        # If the test is not statistically significant (because of, e.g., small number \n",
    "        # of answers or ratings), we don't want to add a bonus.\n",
    "        # If the correlation is significant but negative I assume it is a false positive.\n",
    "        \n",
    "        if not np.isnan(test_tau) and (test_p_value < 0.05 and test_tau > 0):\n",
    "            # start by normalizing the score vector so that values are between 0 and 1\n",
    "            svec = anca_vec - np.min(anca_vec)\n",
    "            svec = svec / float(np.max(svec))\n",
    "            \n",
    "            # normalize the count of ratings per answer so values are between 0 and 1\n",
    "            rvec = num_ratings - np.min(num_ratings)\n",
    "            rvec = rvec / float(np.max(rvec))\n",
    "\n",
    "            # Add a bonus for number of ratings to the score vector. The size \n",
    "            # of the bonus is weighted by the correlation between the two vectors.\n",
    "            wvec = svec + (rvec * test_tau)\n",
    "            \n",
    "        else:\n",
    "            wvec = anca_vec\n",
    "        \n",
    "        # reweight so scores look more intuitive \n",
    "        # (note that this doesn't change the ordering!)\n",
    "        wvec = wvec - np.min(wvec) + 0.15 * np.mean(wvec)\n",
    "        wvec = (wvec / (np.max(wvec) + 0.15 * np.mean(wvec)))\n",
    "            \n",
    "        return wvec\n",
    "        \n",
    "    def mean_rating(self):\n",
    "        \"\"\"Returns the mean score of each item (i.e. the status quo of the platform)\"\"\"\n",
    "        \n",
    "        def nonmissing_mean(arr):\n",
    "            if np.sum(arr >= 0) > 0:\n",
    "                return np.mean(arr[np.where(arr >= 0)])\n",
    "            else:\n",
    "                return 0.0\n",
    "        \n",
    "        return np.apply_along_axis(nonmissing_mean, axis=0, arr=self.matrix)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define synthetic score matrix class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from scipy.special import expit, logit\n",
    "\n",
    "class ScoreMatrixSynthetic(AbstractScoreMatrix):\n",
    "    \"\"\"Generates a rater-by-answer matrix of scores for answers to the question.\n",
    "    Note that -1 indicates missing data.\n",
    "    \n",
    "    Parameters:\n",
    "        n: number of users (though not all of them have to contribute)\n",
    "        rnoise: scaling factor, influences amount of noise in the ratings\n",
    "        qnoise: RdR suggested this. Because the same people are submitting the answers\n",
    "            and rating them, we expect a +ve correlation between the skill of the rater and\n",
    "            the quality of that user's answer. When qnoise is larger, that correlation is\n",
    "            smaller.\n",
    "        punrated: the probability that a user will fail to rate an item\n",
    "        pslacker: the probability that a user will fail to rate any items\n",
    "        plurker: the probability that a user will fail to submit an answer\n",
    "        toprated: if True, the user rates the answers they consider best. If False, they \n",
    "            rate a random selection of the answers.\n",
    "        enoise: each rater has an \"easiness\" bias, i.e. how harsh or easy a rater they are.\n",
    "            This is drawn from a Gaussian with mean zero, and enoise is its standard dev.\n",
    "        diag: if True, users are allowed to rate their own answers; if False (the default)\n",
    "             they can't.\n",
    "             \n",
    "    Note that the overall probabilities are different due to interactions between the \n",
    "    parameters. E.g. if pslacker=1, then none of the items will be rated, even if punrated=0.\n",
    "    These are best considered as tuning parameters. Look at the overall number of ratings in\n",
    "    the output when interpreting results.\n",
    "    \"\"\" \n",
    "    \n",
    "    def __init__(self, n=30, rnoise=0.5, qnoise=0.05, punrated=0.2, \n",
    "                 pslacker=.1, plurker=.2, enoise=.5, toprated=True, diag=False, weights=[]):\n",
    "        self.matrix = self.get_score_matrix(n, rnoise, qnoise, punrated, pslacker, plurker, enoise, toprated, diag)\n",
    "        self.answer_ids = range(self.matrix.shape[1])\n",
    "        self.rater_ids = range(self.matrix.shape[0])\n",
    "        self.subset = self.get_missing_subset()\n",
    "        \n",
    "        self.anca = self.anca_rating()\n",
    "        self.lizzie = self.lizzie_rating()        \n",
    "            \n",
    "        \n",
    "    def get_score_matrix(self, n=30, rnoise=0.5, qnoise=0.05, punrated=0.2, \n",
    "                         pslacker=.1, plurker=.2, enoise=.5, toprated=True, diag=False):\n",
    "        \"\"\"produces user-by-answer score matrix (as a 2D Numpy array)\"\"\"\n",
    "        \n",
    "        # draw from a truncated Gaussian distribution.\n",
    "        # Note that this is a rejection sampling method, will be VERY SLOW if\n",
    "        # sigma is large relative to the distance between lower and upper.\n",
    "        def trunc_gauss(mu=0.5, sigma=1, lower=0, upper=1):\n",
    "            if mu <= lower or mu >= upper:\n",
    "                raise ValueError('mu outside bounds', mu, lower, upper)\n",
    "            \n",
    "            y = -1\n",
    "            while y <= lower or y >= upper:\n",
    "                y = np.random.normal(loc=mu,scale=sigma)\n",
    "            return y\n",
    "        \n",
    "        trunc_gauss_vec = np.vectorize(trunc_gauss)\n",
    "        \n",
    "        # draw ability uniformly between 0 and 1\n",
    "        ability = np.random.random(n)\n",
    "        # draw each rater's \"easiness\" value from a gaussian\n",
    "        easiness = np.random.normal(loc=0, scale=enoise, size=n)\n",
    "        \n",
    "        # add (truncated) Gaussian noise to get quality of answers\n",
    "        quality = trunc_gauss_vec(ability, sigma=qnoise)\n",
    "        \n",
    "        # set up ratings matrix\n",
    "        A = np.empty((n,n), dtype=float)\n",
    "        for i in range(n):\n",
    "            # draw all ratings\n",
    "            ratings_i = trunc_gauss_vec(quality, sigma = (1-ability[i])*rnoise)\n",
    "            # add easiness bias for this rater\n",
    "            ratings_i = expit(logit(ratings_i) + easiness[i])\n",
    "            A[i,:] = ratings_i\n",
    "            \n",
    "            # unrated: draw number of unrated answers\n",
    "            ki = np.random.binomial(n, punrated)\n",
    "            if toprated:\n",
    "                # see that rater's top-rated answers\n",
    "                Aki = np.argsort(A[i,:])[range(ki)]\n",
    "            else: \n",
    "                # see ratings for a random set of answers\n",
    "                Aki = np.random.choice(n, ki, replace=False)\n",
    "            # set unrated elements to -1 (missing)\n",
    "            if len(Aki) > 0:\n",
    "                A[i, Aki] = -1\n",
    "\n",
    "        # can't rate your own answers\n",
    "        if not diag:\n",
    "            np.fill_diagonal(A,-1)\n",
    "\n",
    "        # \"slackers\" are people who don't rate answers\n",
    "        # pick slackers with probability p, remove their rows from the matrix\n",
    "        kslack = np.random.binomial(n, pslacker)\n",
    "        slack = np.random.choice(n, kslack, replace=False)\n",
    "        A = np.delete(A, slack, 0)\n",
    "\n",
    "        # \"lurkers\" are people who don't write answers\n",
    "        # pick lurkers with probability p, remove their columns from the matrix\n",
    "        klurk = np.random.binomial(n, plurker)\n",
    "        lurk = np.random.choice(n, klurk, replace=False)\n",
    "        A = np.delete(A, lurk, 1)\n",
    "        self.quality = np.delete(quality, lurk)\n",
    "        \n",
    "        return A\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ScoreMatrix (given data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from scipy.special import expit, logit\n",
    "\n",
    "class ScoreMatrix(AbstractScoreMatrix):\n",
    "    \"\"\"Takes an numpy array containing a rater-by-answer matrix of scores, and \n",
    "    turns it into a ScoreMatrix object with all the rating aggregation functions.\n",
    "    Note that -1 indicates missing data.\n",
    "    \"\"\" \n",
    "    \n",
    "    def __init__(self, arr):\n",
    "        self.matrix = arr\n",
    "        self.answer_ids = range(self.matrix.shape[1])\n",
    "        self.rater_ids = range(self.matrix.shape[0])\n",
    "        self.subset = self.get_missing_subset()\n",
    "        \n",
    "        self.anca = self.anca_rating()\n",
    "        self.lizzie = self.lizzie_rating()         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_case = np.array([[-1.        ,  0.69598663],\n",
    "       [ 0.56882739, -1.        ],\n",
    "       [ 0.70957079,  0.08803212],\n",
    "       [ 0.5829268 ,  0.51257361]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.91846298,  0.16869728])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[.7, .57, .4],\n",
    "              [.1, .7, .6],\n",
    "              [.2, .5, .6]])\n",
    "\n",
    "\n",
    "test_sm = ScoreMatrix(problematic_case)\n",
    "test_sm.lizzie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.]]\n",
      "[ 0.]\n",
      "[ nan]\n",
      "[ 0.]\n",
      "[ 62.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:140: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "test_sm = ScoreMatrixSynthetic(n=1)\n",
    "\n",
    "mean_diffs = []\n",
    "lizzie_diffs = []\n",
    "\n",
    "print(test_sm.matrix)\n",
    "\n",
    "print(np.round(test_sm.anca_rating() * 100.0))\n",
    "print(np.round(test_sm.lizzie_rating() * 100.0))\n",
    "print(np.round(test_sm.mean_rating() * 100))\n",
    "print(np.round(test_sm.quality * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.244865235818\n",
      "0.354530609104\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(lizzie_diffs))\n",
    "print(np.mean(mean_diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lizzie_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.560165  ,  0.39525403,  0.32496408,  0.71184185,  0.87376629,\n",
       "        0.15022469,  0.67782464,  0.25320031,  0.65055905,  0.92556927,\n",
       "        0.83546673,  0.31114461,  0.57951105,  0.07508036,  0.11850066])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sm.lizzie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bye\n"
     ]
    }
   ],
   "source": [
    "if not (np.isnan(test_rho)):\n",
    "    print(test_rho)\n",
    "else:\n",
    "    print(\"bye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ,  0.21978449,  0.1863181 ],\n",
       "       [-1.        , -1.        ,  0.41789005],\n",
       "       [ 0.0605736 ,  0.27369676,  0.14095515],\n",
       "       [ 0.37586468,  0.14274267, -1.        ]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = ScoreMatrixSynthetic(n=4)\n",
    "b.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.  ,  1.25,  0.  ,  0.  ,  1.  ,  1.  ,  1.75,  0.  ]), 1.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sm = ScoreMatrixSynthetic(n=10, punrated = .8)\n",
    "anca_vec = test_sm.anca\n",
    "anca_min = np.min(anca_vec[np.where(anca_vec > 0.0)])\n",
    "anca_vec, anca_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan for analysis:\n",
    "\n",
    "* In worst case (high noise, few ratings, etc.), and in best case,\n",
    "    * Which methods get the top answer right?\n",
    "    * Which methods have highest Spearman's rho?\n",
    "* What effect does setting topn=True have?\n",
    "* What effect does the easiness bias have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: notes to self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User preference ranking\n",
    "\n",
    "In Chapter 10 of _Who's #1?_, Langville & Meyer describe a ranking method for _products_ (analogous to answers on SWARM) that have been rated by users according to a star rating scale - that is, given an integer score between 1 and 5. \n",
    "\n",
    "### Formulation:\n",
    "\n",
    "If there are $m$ users and $n$ products, the ratings can be turned into an $m$-by-$n$ user-by-product matrix. \n",
    "\n",
    "$$\\mathbf{A} =\n",
    " \\begin{pmatrix}\n",
    "  a_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\n",
    "  a_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  a_{m,1} & a_{m,2} & \\cdots & a_{m,n}\n",
    " \\end{pmatrix}$$\n",
    "\n",
    "For example:\n",
    "\n",
    "|  | $p_1$ | $p_2$ | $p_3$ | $p_4$ | \n",
    "|---|---|---|---|---|\n",
    "| $u_1$ |  3 |   |   |  5 | \n",
    "| $u_2$ |   | 2  | 2  |  4 | \n",
    "| $u_3$ |  3 |   |  3 |   | \n",
    "| $u_4$ |  1 | 4  |   |  5 | \n",
    "| $u_5$ |   |  5 |  2 |   | \n",
    "| $u_6$ |   |   |  1 |  3 | | \n",
    "\n",
    "Note that the ratings are _sparse_ - users do not have to rate all products.\n",
    "\n",
    "Langville & Meyer begin (page 118) by defining an $n$-b-$n$ skew-symmetric matrix, $\\mathbf{K}$, which holds the pairwise differences between the the products:\n",
    "\n",
    "$$\\mathbf{K} =\n",
    " \\begin{pmatrix}\n",
    "  0 & k_{1,2} & \\cdots & k_{1,n} \\\\\n",
    "  k_{2,1} & 0 & \\cdots & k_{2,n} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  k_{n,1} & k_{n,2} & \\cdots & 0\n",
    " \\end{pmatrix}$$\n",
    "\n",
    "Then (page 128) they define the $k_{i,j}$ terms for this application:\n",
    "\n",
    "$$k_{i,j} = -k_{j,i} = \\begin{cases}\n",
    "\\frac{1}{n_{i,j}} \\sum_{u \\in U_i \\cap U_j} a_{u,i} - a_{u,j} & \\mbox{ if } n_{i,j} \\neq 0\\\\\n",
    "0 & \\mbox{ if } n_{i,j} = 0\n",
    "\\end{cases}$$\n",
    "\n",
    "where $U_i$ is the set of users who have rated product $p_i$, so $\\{u: u \\in U_i \\cap U_j\\}$ is the set of users who have rated both products $p_i$ and $p_j$, and $n_{i,j}$ is the number of users in $\\{u: u \\in U_i \\cap U_j\\}$.\n",
    "\n",
    "Once we have $\\mathbf{K}$, finding the overall rating vector $\\mathbf{r}$ is simple. Simply take the row means - that is,\n",
    "\n",
    "$$\\mathbf{r} = \\frac{\\mathbf{Ke}}{n}$$\n",
    "\n",
    "where $\\mathbf{e}$ is a vector of ones.\n",
    "\n",
    "### Properties\n",
    "\n",
    " - User preference ranking is based on pairwise comparisons between items.\n",
    " - It automatically takes into account that some people are \"easy graders\" - the absolute values of their ratings doesn't matter, only the _difference_ in the scores between pairs of items they have rated.\n",
    " - Users must rate at least two items for their ratings to affect the ranking.\n",
    " - Users influence pairwise comparisons whenever they rate both items in the pair. Call the \"power\" of a user the number of distinct pairs of items they have rated. If the user has rated $k$ items, their power is $\\frac{k^2 - k}{2}$.\n",
    " - User preferences are vulnerable to users voting strategically like so: give an extremely high score to _one_ item, and extremely low scores to several other items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
